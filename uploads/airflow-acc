--------------SIMPLE ETL PIPELINE CODE------------------------------
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import mysql.connector

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 7, 16),  # Ngày bắt đầu
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def extract_data(ti):
    url = 'https://jsonplaceholder.typicode.com/posts'
    response = requests.get(url)
    data = response.json()
    # return data
    ti.xcom_push(key='raw_data', value=data)

def transform_data(**kwargs):
    ti = kwargs['ti']
    data = ti.xcom_pull(task_ids='extract_data', key='raw_data')
    transformed_data = [{'title': d['title'], 'body': d['body']} for d in data]
    ti.xcom_push(key='transformed_data', value=transformed_data)
    # return transformed_data

def load_data(**kwargs):
    ti = kwargs['ti']
    transformed_data = ti.xcom_pull(task_ids='transform_data', key='transformed_data')
    db = mysql.connector.connect(
        host='localhost',
        database='mydatabase',
        user='root',
        password='root'
    )
    mycursor = db.cursor()


    for record in transformed_data:
        mycursor.execute(
            "INSERT INTO typicode_database (title, body) VALUES (%s, %s)",
            (record['title'], record['body'])
        )

    db.commit()
    # mycursor.close()
    # db.close()

with DAG(
    'simple_etl_pipeline',
    default_args=default_args,
    description='A simple ETL pipeline',
    schedule_interval=timedelta(days=1),
    catchup=False
) as dag:

    t1 = PythonOperator(
        task_id='extract_data',
        python_callable=extract_data,
        provide_context=True,
    )

    t2 = PythonOperator(
        task_id='transform_data',
        python_callable=transform_data,
        provide_context=True,
    )

    t3 = PythonOperator(
        task_id='load_data',
        python_callable=load_data,
        provide_context=True,
    )

    t1 >> t2 >> t3

=================================================DOCKER COMPOSE.YML=======================================================

version: '3.0'

services:
  play-dice-container:
    image: apache/airflow:2.9.0-python3.11
    volumes:
      - ./dags:/opt/airflow/dags

    ports:
      - 8080:8080

    command: bash -c '(airflow db init && airflow users create --username admin --password admin --firstname viet --lastname lam --role Admin --email vietlam050607@gmail.com); airflow webserver & airflow scheduler'

volumes:
  dags:
